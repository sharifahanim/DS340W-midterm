{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xGSByoA5tAv"
      },
      "source": [
        "### Set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h67wCYl5pbfT"
      },
      "source": [
        "Importing the data is taken care by kaggle.<br>\n",
        "Just upload your kaggle.json API token file(You can get it from your accounts page on Kaggle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "jLIULOEe4HKQ",
        "outputId": "78d186c2-221f-4bc2-eb24-0c582a21206a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0b5e693e-1421-4d74-989b-6b41e26b0920\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0b5e693e-1421-4d74-989b-6b41e26b0920\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle (1).json': b'{\"username\":\"sharifahrodziahhanim\",\"key\":\"64267733de5fb9f88a4aeede1c343330\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp_6D9sKe91G",
        "outputId": "9ea05ccc-946a-427f-fe47-e06eec060a84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from geopandas) (2.0.2)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from geopandas) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from geopandas) (24.2)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from geopandas) (2.2.2)\n",
            "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from geopandas) (3.7.1)\n",
            "Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from geopandas) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->geopandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->geopandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->geopandas) (2025.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pyogrio>=0.7.2->geopandas) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->geopandas) (1.17.0)\n",
            "Requirement already satisfied: cartopy in /usr/local/lib/python3.11/dist-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from cartopy) (3.10.0)\n",
            "Requirement already satisfied: shapely>=1.8 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.1.0)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from cartopy) (24.2)\n",
            "Requirement already satisfied: pyshp>=2.3 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.3.1)\n",
            "Requirement already satisfied: pyproj>=3.3.1 in /usr/local/lib/python3.11/dist-packages (from cartopy) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pyproj>=3.3.1->cartopy) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->cartopy) (1.17.0)\n",
            "Requirement already satisfied: mplleaflet in /usr/local/lib/python3.11/dist-packages (0.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mplleaflet) (3.1.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from mplleaflet) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mplleaflet) (3.0.2)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.19.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install geopandas\n",
        "!pip install cartopy\n",
        "!pip3 install mplleaflet\n",
        "! pip install -q kaggle\n",
        "! pip install torch-geometric\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "piVTo6C4kwjl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from matplotlib import cm, colors\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.inspection import permutation_importance\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0Iap-wp4VWu",
        "outputId": "89c44d2f-0758-431d-8391-935c2faa4f88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UKDiV_cS4Wrj"
      },
      "outputs": [],
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1DrUIO_f4bMZ"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK6BUd_Q4eBT",
        "outputId": "0bfb6f91-db65-46c4-ff8f-84968c27c7b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                               title                                                    size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "----------------------------------------------------------------  -------------------------------------------------  ----------  --------------------------  -------------  ---------  ---------------  \n",
            "adilshamim8/student-depression-dataset                            Student Depression Dataset                             467020  2025-03-13 03:12:30.423000          18464        292  1.0              \n",
            "zahidmughal2343/amazon-sales-2025                                 Amazon Sales 2025                                        3617  2025-04-03 22:08:13.607000           3831         47  1.0              \n",
            "lucass0s0/polycystic-ovary-syndrome-pcos                          Polycystic Ovary Syndrome PCOS                          19089  2025-04-14 12:14:58.850000            548         23  1.0              \n",
            "qucwang/hotel-bookings-analysis-dataset                           Hotel bookings demand                                 1308365  2025-04-06 15:23:40.413000           1344         26  1.0              \n",
            "khushikyad001/covid-19-global-dataset                             Covid-19 Global Dataset                                482555  2025-04-12 18:25:15.140000            993         24  1.0              \n",
            "shantanugarg274/sales-dataset                                     Sales Dataset                                           24621  2025-03-22 16:21:02.010000           2473         41  1.0              \n",
            "atharvasoundankar/chocolate-sales                                 Chocolate Sales Data 📊🍫                                 14473  2025-03-19 03:51:40.270000          24764        385  1.0              \n",
            "atharvasoundankar/smart-farming-sensor-data-for-yield-prediction  🌾 Smart Farming Sensor Data for Yield Prediction        28385  2025-04-15 10:04:19.530000            669         22  1.0              \n",
            "atharvasoundankar/impact-of-ai-on-digital-media-2020-2025         🌍 Impact of AI on Digital Media (2020-2025)              5812  2025-04-03 09:12:25.070000           3984         70  1.0              \n",
            "ethancratchley/email-phishing-dataset                             Email Phishing Dataset                                3336668  2025-04-01 01:23:06.577000           1030         23  1.0              \n",
            "atharvasoundankar/fashion-retail-sales                            🛍️ Fashion Retail Sales Dataset                         31656  2025-04-01 05:05:48.020000           2223         34  1.0              \n",
            "ricardobj/electric-vehicle-population                             Electric Vehicle Population                           7925319  2025-04-10 10:47:13.150000           1613         28  1.0              \n",
            "danielcalvoglez/us-tariffs-2025                                   US Tariffs 2025                                          9108  2025-04-07 14:14:04.080000           1371         28  1.0              \n",
            "marshalpatel3558/diabetes-prediction-dataset                      Diabetes_prediction_dataset                            326543  2025-03-21 19:50:19.707000           1827         30  1.0              \n",
            "shriyashjagtap/esg-and-financial-performance-dataset              🌍 ESG & Financial Performance Dataset                  387627  2025-03-30 14:51:59.937000           1027         22  1.0              \n",
            "abdmoiz/walmart-stock-data-2025                                   Walmart Stock Data 2025                                178869  2025-03-26 00:47:09.017000           1392         28  1.0              \n",
            "devdope/900k-spotify                                              🎧 900K+ Spotify Songs with Lyrics,Emotions & More   689091572  2025-04-09 04:06:17.387000            965         35  1.0              \n",
            "jacopoferretti/child-weight-at-birth-and-gestation-details        Child Weight at Birth and Gestation Details             11309  2025-04-18 23:13:11.427000            466         25  1.0              \n",
            "adilshamim8/math-students                                         Math-Students Performance Data                           7367  2025-04-02 02:47:02.760000           2169         38  1.0              \n",
            "vinothkannaece/mobiles-and-laptop-sales-data                      Mobiles & laptop Sales Data                           3242055  2025-03-24 05:03:52.657000           2774         40  1.0              \n"
          ]
        }
      ],
      "source": [
        "! kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxQu_EzB5GBX",
        "outputId": "416ae404-0622-4e17-83ea-7ae835987a4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/tsiaras/uk-road-safety-accidents-and-vehicles\n",
            "License(s): DbCL-1.0\n",
            "uk-road-safety-accidents-and-vehicles.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d tsiaras/uk-road-safety-accidents-and-vehicles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpGogg075K4l",
        "outputId": "f0a60b48-e4d1-4f26-e0ae-ed6ec4e765b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘train’: File exists\n",
            "Archive:  uk-road-safety-accidents-and-vehicles.zip\n",
            "replace train/Accident_Information.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: train/Accident_Information.csv  \n",
            "  inflating: train/Vehicle_Information.csv  \n"
          ]
        }
      ],
      "source": [
        "! mkdir train\n",
        "! unzip uk-road-safety-accidents-and-vehicles.zip -d train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZTbRPdmuGOy",
        "outputId": "7b652559-0628-4474-ddf1-2a13dff0773f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train/Accident_Information.csv\n",
            "train/Vehicle_Information.csv\n"
          ]
        }
      ],
      "source": [
        "for dirname, _, filenames in os.walk('train'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSgumlMturld",
        "outputId": "8e31ceaf-48f9-439a-c850-aefb8b4b6542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-0ace3edffbf7>:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  accidents=pd.read_csv(\"train/Accident_Information.csv\")\n"
          ]
        }
      ],
      "source": [
        "accidents=pd.read_csv(\"train/Accident_Information.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpDS5SOqdN9E"
      },
      "source": [
        "## Feature engineering & data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F_qUKmJdZ4j",
        "outputId": "6ccf45a7-a16b-429c-ac75-082875d0f07f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values before preprocessing:\n",
            "Latitude              174\n",
            "Longitude             175\n",
            "Speed_limit            37\n",
            "Weather_Conditions      0\n",
            "Road_Type               0\n",
            "Junction_Detail         0\n",
            "Day_of_Week             0\n",
            "Number_of_Vehicles      0\n",
            "Accident_Severity       0\n",
            "dtype: int64\n",
            "\n",
            "Missing values after preprocessing:\n",
            "Latitude              0\n",
            "Longitude             0\n",
            "Speed_limit           0\n",
            "Weather_Conditions    0\n",
            "Road_Type             0\n",
            "Junction_Detail       0\n",
            "Day_of_Week           0\n",
            "Number_of_Vehicles    0\n",
            "Accident_Severity     0\n",
            "dtype: int64\n",
            "\n",
            "Number of samples after preprocessing: 2047256\n",
            "\n",
            "Class distribution of Accident_Severity:\n",
            "Accident_Severity\n",
            "Slight     1734548\n",
            "Serious     286339\n",
            "Fatal        26369\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Checking for NaNs in X:\n",
            "0 missing values in X\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "import geopandas as gpd\n",
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "\n",
        "# Load Data with explicit dtype to avoid DtypeWarning\n",
        "accidents = pd.read_csv(\n",
        "    \"train/Accident_Information.csv\",\n",
        "    dtype={'Accident_Index': str},\n",
        "    low_memory=False\n",
        ")\n",
        "\n",
        "# Feature Engineering & Preprocessing\n",
        "features = [\n",
        "    'Latitude', 'Longitude', 'Speed_limit', 'Weather_Conditions',\n",
        "    'Road_Type', 'Junction_Detail', 'Day_of_Week', 'Number_of_Vehicles'\n",
        "]\n",
        "target = 'Accident_Severity'\n",
        "\n",
        "# Select relevant columns\n",
        "accidents = accidents[features + [target]].copy()\n",
        "\n",
        "# Debug: Check missing values\n",
        "print(\"Missing values before preprocessing:\")\n",
        "print(accidents.isna().sum())\n",
        "\n",
        "# Handle missing values (avoid inplace operations)\n",
        "accidents['Speed_limit'] = accidents['Speed_limit'].fillna(accidents['Speed_limit'].median())\n",
        "accidents['Weather_Conditions'] = accidents['Weather_Conditions'].fillna('Fine no high winds')\n",
        "accidents['Road_Type'] = accidents['Road_Type'].fillna('Single carriageway')\n",
        "accidents['Junction_Detail'] = accidents['Junction_Detail'].fillna('Not at junction or within 20 metres')\n",
        "accidents['Day_of_Week'] = accidents['Day_of_Week'].fillna(accidents['Day_of_Week'].mode()[0])\n",
        "accidents['Number_of_Vehicles'] = accidents['Number_of_Vehicles'].fillna(accidents['Number_of_Vehicles'].median())\n",
        "accidents['Latitude'] = accidents['Latitude'].fillna(accidents['Latitude'].median())\n",
        "accidents['Longitude'] = accidents['Longitude'].fillna(accidents['Longitude'].median())\n",
        "\n",
        "# Drop rows with missing target\n",
        "accidents = accidents.dropna(subset=[target]).reset_index(drop=True)\n",
        "\n",
        "# Debug: Verify no missing values remain\n",
        "print(\"\\nMissing values after preprocessing:\")\n",
        "print(accidents.isna().sum())\n",
        "print(f\"\\nNumber of samples after preprocessing: {len(accidents)}\")\n",
        "\n",
        "# Debug: Check class distribution\n",
        "print(\"\\nClass distribution of Accident_Severity:\")\n",
        "print(accidents[target].value_counts())\n",
        "\n",
        "# Encode categorical features\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "cat_features = ['Weather_Conditions', 'Road_Type', 'Junction_Detail', 'Day_of_Week']\n",
        "cat_encoded = encoder.fit_transform(accidents[cat_features])\n",
        "cat_cols = encoder.get_feature_names_out(cat_features)\n",
        "cat_df = pd.DataFrame(cat_encoded, columns=cat_cols, index=accidents.index)\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "num_features = ['Latitude', 'Longitude', 'Speed_limit', 'Number_of_Vehicles']\n",
        "num_scaled = scaler.fit_transform(accidents[num_features])\n",
        "num_df = pd.DataFrame(num_scaled, columns=num_features, index=accidents.index)\n",
        "\n",
        "# Combine features\n",
        "X = pd.concat([num_df, cat_df], axis=1)\n",
        "y = LabelEncoder().fit_transform(accidents[target])\n",
        "\n",
        "# Debug: Verify X has no NaNs\n",
        "print(\"\\nChecking for NaNs in X:\")\n",
        "print(X.isna().sum().sum(), \"missing values in X\")\n",
        "\n",
        "# Convert to tensors\n",
        "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvdOUdfPeYRf"
      },
      "source": [
        "## Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ogx-gmKeeaY9"
      },
      "outputs": [],
      "source": [
        "# Train-Validation-Test Split\n",
        "train_idx, temp_idx, y_train, y_temp = train_test_split(\n",
        "    np.arange(X_tensor.size(0)), y_tensor.numpy(), test_size=0.3, random_state=42, stratify=y_tensor.numpy()\n",
        ")\n",
        "val_idx, test_idx = train_test_split(\n",
        "    temp_idx, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "train_mask = torch.zeros(X_tensor.size(0), dtype=torch.bool)\n",
        "val_mask = torch.zeros(X_tensor.size(0), dtype=torch.bool)\n",
        "test_mask = torch.zeros(X_tensor.size(0), dtype=torch.bool)\n",
        "train_mask[train_idx] = True\n",
        "val_mask[val_idx] = True\n",
        "test_mask[test_idx] = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHoaNwbRd8ex"
      },
      "source": [
        "## Graph Construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "w5uzFmbgd-o-"
      },
      "outputs": [],
      "source": [
        "# Create Edges using Spatial Nearest Neighbors (KNN Graph)\n",
        "n_neighbors = 5\n",
        "adj_matrix = kneighbors_graph(X[['Latitude', 'Longitude']], n_neighbors, mode='connectivity', include_self=False)\n",
        "edge_index = torch.tensor(np.array(adj_matrix.nonzero()), dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiTZ66qXCI1t"
      },
      "source": [
        "# GNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HuB7GtUjkBwE"
      },
      "outputs": [],
      "source": [
        "# GNN Model\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim // 2)\n",
        "        self.conv3 = GCNConv(hidden_dim // 2, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Initialize Model\n",
        "input_dim = X.shape[1]\n",
        "hidden_dim = 64\n",
        "output_dim = len(np.unique(y))\n",
        "model = GNNModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Class Weights\n",
        "classes = np.unique(y)\n",
        "cw = compute_class_weight('balanced', classes=classes, y=y)\n",
        "class_weights = torch.tensor(cw, dtype=torch.float32)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNp2h7HJemgQ"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "88AfRqhVeoBo"
      },
      "outputs": [],
      "source": [
        "# Training with Early Stopping\n",
        "def train_model(model, X, y, edge_index, train_mask, val_mask, epochs=10, patience=10):\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    model.train()\n",
        "\n",
        "    # Lists to store metrics\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(X, edge_index)\n",
        "        loss = criterion(out[train_mask], y[train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_out = model(X, edge_index)\n",
        "            val_loss = criterion(val_out[val_mask], y[val_mask])\n",
        "            val_acc = (val_out[val_mask].argmax(dim=1) == y[val_mask]).float().mean().item()\n",
        "        model.train()\n",
        "\n",
        "        # Store metrics\n",
        "        train_losses.append(loss.item())\n",
        "        val_losses.append(val_loss.item())\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "    return train_losses, val_losses, val_accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1canpuzYeJ_Q"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWA-VDb-eLqr",
        "outputId": "6ce43ce9-8e08-4f27-af6f-7727f13ee117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 1.1790, Val Loss: 1.1582, Val Acc: 0.0867\n",
            "Epoch 2/10, Train Loss: 1.1644, Val Loss: 1.1455, Val Acc: 0.0847\n",
            "Epoch 3/10, Train Loss: 1.1510, Val Loss: 1.1343, Val Acc: 0.0826\n",
            "Epoch 4/10, Train Loss: 1.1390, Val Loss: 1.1245, Val Acc: 0.0798\n",
            "Epoch 5/10, Train Loss: 1.1299, Val Loss: 1.1161, Val Acc: 0.0762\n",
            "Epoch 6/10, Train Loss: 1.1207, Val Loss: 1.1090, Val Acc: 0.0730\n",
            "Epoch 7/10, Train Loss: 1.1134, Val Loss: 1.1031, Val Acc: 0.0759\n",
            "Epoch 8/10, Train Loss: 1.1072, Val Loss: 1.0982, Val Acc: 0.0995\n",
            "Epoch 9/10, Train Loss: 1.1024, Val Loss: 1.0943, Val Acc: 0.1510\n",
            "Epoch 10/10, Train Loss: 1.0979, Val Loss: 1.0911, Val Acc: 0.2114\n",
            "\n",
            "Test Accuracy: 0.2125\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.02      0.61      0.03      3956\n",
            "           1       0.13      0.35      0.19     42951\n",
            "           2       0.87      0.18      0.30    260182\n",
            "\n",
            "    accuracy                           0.21    307089\n",
            "   macro avg       0.34      0.38      0.18    307089\n",
            "weighted avg       0.75      0.21      0.28    307089\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluation\n",
        "def evaluate(model, X, y, edge_index, mask, set_name=\"Test\"):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(X, edge_index)\n",
        "        preds = out[mask].argmax(dim=1)\n",
        "        actuals = y[mask]\n",
        "        acc = (preds == actuals).float().mean().item()\n",
        "        print(f\"\\n{set_name} Accuracy: {acc:.4f}\")\n",
        "        print(classification_report(actuals.cpu(), preds.cpu(), zero_division=0))\n",
        "    return preds.cpu().numpy(), actuals.cpu().numpy()\n",
        "\n",
        "# Train and Evaluate\n",
        "train_losses, val_losses, val_accuracies = train_model(model, X_tensor, y_tensor, edge_index, train_mask, val_mask)\n",
        "test_preds, test_actuals = evaluate(model, X_tensor, y_tensor, edge_index, test_mask, \"Test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeqsBrHTevRG"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRasI2ranS-I",
        "outputId": "587253cb-5559-4053-e8d0-2b30f6900f35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-f97c79445d5d>:7: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.barplot(x=class_counts.index, y=class_counts.values, palette='viridis')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution plot saved as 'class_distribution.png'\n"
          ]
        }
      ],
      "source": [
        "# Set Seaborn style for better aesthetics\n",
        "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
        "\n",
        "# 1. Class Distribution Bar Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "class_counts = accidents['Accident_Severity'].value_counts()\n",
        "sns.barplot(x=class_counts.index, y=class_counts.values, palette='viridis')\n",
        "plt.title('Distribution of Accident Severity Classes', fontsize=14)\n",
        "plt.xlabel('Accident Severity', fontsize=12)\n",
        "plt.ylabel('Number of Accidents', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('class_distribution.png', dpi=300)\n",
        "plt.close()\n",
        "print(\"Class distribution plot saved as 'class_distribution.png'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZtnxm6ynVyR",
        "outputId": "e01f2146-eed1-4815-beb4-d3b9cda76bf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix heatmap saved as 'confusion_matrix.png'\n"
          ]
        }
      ],
      "source": [
        "# 2. Confusion Matrix Heatmap\n",
        "cm = confusion_matrix(test_actuals, test_preds)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=np.unique(accidents['Accident_Severity']),\n",
        "            yticklabels=np.unique(accidents['Accident_Severity']))\n",
        "plt.title('Confusion Matrix for Test Set', fontsize=14)\n",
        "plt.xlabel('Predicted Severity', fontsize=12)\n",
        "plt.ylabel('Actual Severity', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix.png', dpi=300)\n",
        "plt.close()\n",
        "print(\"Confusion matrix heatmap saved as 'confusion_matrix.png'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rlm92fwnh_-",
        "outputId": "f7b511f7-36df-4e53-bafa-74b8c4a876fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training curves plot saved as 'training_curves.png'\n"
          ]
        }
      ],
      "source": [
        "# 5. Training and Validation Loss/Accuracy Curves\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "ax1.set_xlabel('Epoch', fontsize=12)\n",
        "ax1.set_ylabel('Loss', fontsize=12, color='tab:blue')\n",
        "ax1.plot(train_losses, label='Train Loss', color='tab:blue')\n",
        "ax1.plot(val_losses, label='Validation Loss', color='tab:cyan')\n",
        "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
        "ax1.legend(loc='upper left')\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel('Validation Accuracy', fontsize=12, color='tab:red')\n",
        "ax2.plot(val_accuracies, label='Validation Accuracy', color='tab:red')\n",
        "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "plt.title('Training and Validation Metrics', fontsize=14)\n",
        "fig.tight_layout()\n",
        "plt.savefig('training_curves.png', dpi=300)\n",
        "plt.close()\n",
        "print(\"Training curves plot saved as 'training_curves.png'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGUT07LjL9j_",
        "outputId": "99ab9124-c60e-4670-8271-c83b2f03d8d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Heatmap saved as 'accident_hotspots.html'\n"
          ]
        }
      ],
      "source": [
        "# Visualization (Heatmap with Folium)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred_labels = model(X_tensor, edge_index).argmax(dim=1).numpy()\n",
        "\n",
        "# Create a heatmap of predicted hotspots\n",
        "gdf = gpd.GeoDataFrame(\n",
        "    accidents,\n",
        "    geometry=gpd.points_from_xy(accidents.Longitude, accidents.Latitude),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "gdf['pred'] = pred_labels\n",
        "\n",
        "# Focus on severe accidents (e.g., class 0 or 1)\n",
        "severe = gdf[gdf['pred'].isin([0, 1])]\n",
        "m = folium.Map(location=[gdf['Latitude'].mean(), gdf['Longitude'].mean()], zoom_start=6)\n",
        "heat_data = [[row['Latitude'], row['Longitude']] for _, row in severe.iterrows()]\n",
        "HeatMap(heat_data, radius=10).add_to(m)\n",
        "m.save(\"accident_hotspots.html\")\n",
        "print(\"Heatmap saved as 'accident_hotspots.html'\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}